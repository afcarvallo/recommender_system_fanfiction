{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math, re, string, requests, json\n",
    "from itertools import product\n",
    "from inspect import getsourcefile\n",
    "from os.path import abspath, join, dirname\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random\n",
    "%matplotlib inline \n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets_recsys/reviews.csv', sep=';', encoding='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reviews: 132711\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>story_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3522972</td>\n",
       "      <td>11-04-14</td>\n",
       "      <td>Why is this so adorable? I blame the ending. S...</td>\n",
       "      <td>10000529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3700017</td>\n",
       "      <td>07-02-14</td>\n",
       "      <td>Oh my goodness. Eliot and Clint working togeth...</td>\n",
       "      <td>10000529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>462777</td>\n",
       "      <td>1/30/2014</td>\n",
       "      <td>This was an awesome piece! I like how you bro...</td>\n",
       "      <td>10000529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7455526</td>\n",
       "      <td>05-abr</td>\n",
       "      <td>wtf</td>\n",
       "      <td>10001611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6451578</td>\n",
       "      <td>12/24/2016</td>\n",
       "      <td>I just read this a second time &amp; I still think...</td>\n",
       "      <td>10001611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_name review_date  \\\n",
       "0        3522972    11-04-14   \n",
       "1        3700017    07-02-14   \n",
       "2         462777   1/30/2014   \n",
       "3        7455526      05-abr   \n",
       "4        6451578  12/24/2016   \n",
       "\n",
       "                                         review_text  story_id  \n",
       "0  Why is this so adorable? I blame the ending. S...  10000529  \n",
       "1  Oh my goodness. Eliot and Clint working togeth...  10000529  \n",
       "2   This was an awesome piece! I like how you bro...  10000529  \n",
       "3                                                wtf  10001611  \n",
       "4  I just read this a second time & I still think...  10001611  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('total reviews: {}'.format(len(df['story_id'])))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentIntensityAnalyzer(object):\n",
    "    \"\"\"\n",
    "    Give a sentiment intensity score to sentences.\n",
    "    \"\"\"\n",
    "    def __init__(self, lexicon_file=\"vader_lexicon.txt\"):\n",
    "        _this_module_file_path_ = abspath(getsourcefile(lambda:0))\n",
    "        lexicon_full_filepath = join(dirname(_this_module_file_path_), lexicon_file)\n",
    "        with open(lexicon_full_filepath) as f:\n",
    "            self.lexicon_full_filepath = f.read()\n",
    "        self.lexicon = self.make_lex_dict()\n",
    "\n",
    "    def make_lex_dict(self):\n",
    "        \"\"\"\n",
    "        Convert lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        lex_dict = {}\n",
    "        for line in self.lexicon_full_filepath.split('\\n'):\n",
    "            (word, measure) = line.strip().split('\\t')[0:2]\n",
    "            lex_dict[word] = float(measure)\n",
    "        return lex_dict\n",
    "\n",
    "    def polarity_scores(self, text):\n",
    "        \"\"\"\n",
    "        Return a float for sentiment strength based on the input text.\n",
    "        Positive values are positive valence, negative value are negative\n",
    "        valence.\n",
    "        \"\"\"\n",
    "        sentitext = SentiText(text)\n",
    "        #text, words_and_emoticons, is_cap_diff = self.preprocess(text)\n",
    "\n",
    "        sentiments = []\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        for item in words_and_emoticons:\n",
    "            valence = 0\n",
    "            i = words_and_emoticons.index(item)\n",
    "            if (i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and \\\n",
    "                words_and_emoticons[i+1].lower() == \"of\") or \\\n",
    "                item.lower() in BOOSTER_DICT:\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "\n",
    "            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n",
    "\n",
    "        sentiments = self._but_check(words_and_emoticons, sentiments)\n",
    "        \n",
    "        valence_dict = self.score_valence(sentiments, text)\n",
    "\n",
    "        return valence_dict\n",
    "\n",
    "    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n",
    "        is_cap_diff = sentitext.is_cap_diff\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        item_lowercase = item.lower()\n",
    "        if item_lowercase in self.lexicon:\n",
    "            #get the sentiment valence\n",
    "            valence = self.lexicon[item_lowercase]\n",
    "\n",
    "            #check if sentiment laden word is in ALL CAPS (while others aren't)\n",
    "            if item.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    valence += C_INCR\n",
    "                else:\n",
    "                    valence -= C_INCR\n",
    "\n",
    "            for start_i in range(0,3):\n",
    "                if i > start_i and words_and_emoticons[i-(start_i+1)].lower() not in self.lexicon:\n",
    "                    # dampen the scalar modifier of preceding words and emoticons\n",
    "                    # (excluding the ones that immediately preceed the item) based\n",
    "                    # on their distance from the current item.\n",
    "                    s = scalar_inc_dec(words_and_emoticons[i-(start_i+1)], valence, is_cap_diff)\n",
    "                    if start_i == 1 and s != 0:\n",
    "                        s = s*0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s*0.9\n",
    "                    valence = valence+s\n",
    "                    valence = self._never_check(valence, words_and_emoticons, start_i, i)\n",
    "                    if start_i == 2:\n",
    "                        valence = self._idioms_check(valence, words_and_emoticons, i)\n",
    "\n",
    "                        # future work: consider other sentiment-laden idioms\n",
    "                        # other_idioms =\n",
    "                        # {\"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
    "                        #  \"upper hand\": 1, \"break a leg\": 2,\n",
    "                        #  \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
    "                        #  \"on the ball\": 2,\"under the weather\": -2}\n",
    "\n",
    "            valence = self._least_check(valence, words_and_emoticons, i)\n",
    "\n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "\n",
    "    def _least_check(self, valence, words_and_emoticons, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if i > 1 and words_and_emoticons[i-1].lower() not in self.lexicon \\\n",
    "           and words_and_emoticons[i-1].lower() == \"least\":\n",
    "            if words_and_emoticons[i-2].lower() != \"at\" and words_and_emoticons[i-2].lower() != \"very\":\n",
    "                valence = valence*N_SCALAR\n",
    "        elif i > 0 and words_and_emoticons[i-1].lower() not in self.lexicon \\\n",
    "             and words_and_emoticons[i-1].lower() == \"least\":\n",
    "            valence = valence*N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _but_check(self, words_and_emoticons, sentiments):\n",
    "        # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "        if 'but' in words_and_emoticons or 'BUT' in words_and_emoticons:\n",
    "            try:\n",
    "                bi = words_and_emoticons.index('but')\n",
    "            except ValueError:\n",
    "                bi = words_and_emoticons.index('BUT')\n",
    "            for sentiment in sentiments:\n",
    "                si = sentiments.index(sentiment)\n",
    "                if si < bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment*0.5)\n",
    "                elif si > bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment*1.5)\n",
    "        return sentiments\n",
    "\n",
    "    def _idioms_check(self, valence, words_and_emoticons, i):\n",
    "        onezero = \"{0} {1}\".format(words_and_emoticons[i-1], words_and_emoticons[i])\n",
    "\n",
    "        twoonezero = \"{0} {1} {2}\".format(words_and_emoticons[i-2],\n",
    "                                       words_and_emoticons[i-1], words_and_emoticons[i])\n",
    "\n",
    "        twoone = \"{0} {1}\".format(words_and_emoticons[i-2], words_and_emoticons[i-1])\n",
    "\n",
    "        threetwoone = \"{0} {1} {2}\".format(words_and_emoticons[i-3],\n",
    "                                        words_and_emoticons[i-2], words_and_emoticons[i-1])\n",
    "\n",
    "        threetwo = \"{0} {1}\".format(words_and_emoticons[i-3], words_and_emoticons[i-2])\n",
    "\n",
    "        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n",
    "\n",
    "        for seq in sequences:\n",
    "            if seq in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[seq]\n",
    "                break\n",
    "\n",
    "        if len(words_and_emoticons)-1 > i:\n",
    "            zeroone = \"{0} {1}\".format(words_and_emoticons[i], words_and_emoticons[i+1])\n",
    "            if zeroone in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroone]\n",
    "        if len(words_and_emoticons)-1 > i+1:\n",
    "            zeroonetwo = \"{0} {1} {2}\".format(words_and_emoticons[i], words_and_emoticons[i+1], words_and_emoticons[i+2])\n",
    "            if zeroonetwo in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroonetwo]\n",
    "\n",
    "        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "        if threetwo in BOOSTER_DICT or twoone in BOOSTER_DICT:\n",
    "            valence = valence+B_DECR\n",
    "        return valence\n",
    "\n",
    "    def _never_check(self, valence, words_and_emoticons, start_i, i):\n",
    "        if start_i == 0:\n",
    "            if negated([words_and_emoticons[i-1]]):\n",
    "                    valence = valence*N_SCALAR\n",
    "        if start_i == 1:\n",
    "            if words_and_emoticons[i-2] == \"never\" and\\\n",
    "               (words_and_emoticons[i-1] == \"so\" or\n",
    "                words_and_emoticons[i-1] == \"this\"):\n",
    "                valence = valence*1.5\n",
    "            elif negated([words_and_emoticons[i-(start_i+1)]]):\n",
    "                valence = valence*N_SCALAR\n",
    "        if start_i == 2:\n",
    "            if words_and_emoticons[i-3] == \"never\" and \\\n",
    "               (words_and_emoticons[i-2] == \"so\" or words_and_emoticons[i-2] == \"this\") or \\\n",
    "               (words_and_emoticons[i-1] == \"so\" or words_and_emoticons[i-1] == \"this\"):\n",
    "                valence = valence*1.25\n",
    "            elif negated([words_and_emoticons[i-(start_i+1)]]):\n",
    "                valence = valence*N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _punctuation_emphasis(self, sum_s, text):\n",
    "        # add emphasis from exclamation points and question marks\n",
    "        ep_amplifier = self._amplify_ep(text)\n",
    "        qm_amplifier = self._amplify_qm(text)\n",
    "        punct_emph_amplifier = ep_amplifier+qm_amplifier\n",
    "        return punct_emph_amplifier\n",
    "\n",
    "    def _amplify_ep(self, text):\n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = text.count(\"!\")\n",
    "        if ep_count > 4:\n",
    "            ep_count = 4\n",
    "        # (empirically derived mean sentiment intensity rating increase for\n",
    "        # exclamation points)\n",
    "        ep_amplifier = ep_count*0.292\n",
    "        return ep_amplifier\n",
    "\n",
    "    def _amplify_qm(self, text):\n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = text.count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3:\n",
    "                # (empirically derived mean sentiment intensity rating increase for\n",
    "                # question marks)\n",
    "                qm_amplifier = qm_count*0.18\n",
    "            else:\n",
    "                qm_amplifier = 0.96\n",
    "        return qm_amplifier\n",
    "\n",
    "    def _sift_sentiment_scores(self, sentiments):\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) +1) # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) -1) # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count\n",
    "\n",
    "    def score_valence(self, sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "            # compute and add emphasis from punctuation in text\n",
    "            punct_emph_amplifier = self._punctuation_emphasis(sum_s, text)\n",
    "            if sum_s > 0:\n",
    "                sum_s += punct_emph_amplifier\n",
    "            elif  sum_s < 0:\n",
    "                sum_s -= punct_emph_amplifier\n",
    "\n",
    "            compound = normalize(sum_s)\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n",
    "\n",
    "            if pos_sum > math.fabs(neg_sum):\n",
    "                pos_sum += (punct_emph_amplifier)\n",
    "            elif pos_sum < math.fabs(neg_sum):\n",
    "                neg_sum -= (punct_emph_amplifier)\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = \\\n",
    "            {\"neg\" : round(neg, 3),\n",
    "             \"neu\" : round(neu, 3),\n",
    "             \"pos\" : round(pos, 3),\n",
    "             \"compound\" : round(compound, 4)}\n",
    "\n",
    "        return sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    neg_words = []\n",
    "    neg_words.extend(NEGATE)\n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            return True\n",
    "    if include_nt:\n",
    "        for word in input_words:\n",
    "            if \"n't\" in word:\n",
    "                return True\n",
    "    if \"least\" in input_words:\n",
    "        i = input_words.index(\"least\")\n",
    "        if i > 0 and input_words[i-1] != \"at\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score/math.sqrt((score*score) + alpha)\n",
    "    if norm_score < -1.0: \n",
    "        return -1.0\n",
    "    elif norm_score > 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return norm_score\n",
    "\n",
    "\n",
    "def allcap_differential(words):\n",
    "    \"\"\"\n",
    "    Check whether just some words in the input are ALL CAPS\n",
    "    :param list words: The words to inspect\n",
    "    :returns: `True` if some but not all items in `words` are ALL CAPS\n",
    "    \"\"\"\n",
    "    is_different = False\n",
    "    allcap_words = 0\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            allcap_words += 1\n",
    "    cap_differential = len(words) - allcap_words\n",
    "    if cap_differential > 0 and cap_differential < len(words):\n",
    "        is_different = True\n",
    "    return is_different\n",
    "\n",
    "\n",
    "def scalar_inc_dec(word, valence, is_cap_diff):\n",
    "    \"\"\"\n",
    "    Check if the preceding words increase, decrease, or negate/nullify the\n",
    "    valence\n",
    "    \"\"\"\n",
    "    scalar = 0.0\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in BOOSTER_DICT:\n",
    "        scalar = BOOSTER_DICT[word_lower]\n",
    "        if valence < 0:\n",
    "            scalar *= -1\n",
    "        #check if booster/dampener word is in ALLCAPS (while others aren't)\n",
    "        if word.isupper() and is_cap_diff:\n",
    "            if valence > 0:\n",
    "                scalar += C_INCR\n",
    "            else: scalar -= C_INCR\n",
    "    return scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REGEX_REMOVE_PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for booster words)\n",
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for using\n",
    "# ALLCAPs to emphasize a word)\n",
    "C_INCR = 0.733\n",
    "\n",
    "N_SCALAR = -0.74\n",
    "\n",
    "# for removing punctuation\n",
    "REGEX_REMOVE_PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "PUNC_LIST = [\".\", \"!\", \"?\", \",\", \";\", \":\", \"-\", \"'\", \"\\\"\",\n",
    "             \"!!\", \"!!!\", \"??\", \"???\", \"?!?\", \"!?!\", \"?!?!\", \"!?!?\"]\n",
    "NEGATE = \\\n",
    "[\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    " \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "# booster/dampener 'intensifiers' or 'degree adverbs'\n",
    "# http://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
    "\n",
    "BOOSTER_DICT = \\\n",
    "{\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR, \"completely\": B_INCR, \"considerably\": B_INCR,\n",
    " \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormously\": B_INCR,\n",
    " \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptionally\": B_INCR, \"extremely\": B_INCR,\n",
    " \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR,\n",
    " \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR, \"fucking\": B_INCR,\n",
    " \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR, \"incredibly\": B_INCR,\n",
    " \"intensely\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
    " \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
    " \"so\": B_INCR, \"substantially\": B_INCR,\n",
    " \"thoroughly\": B_INCR, \"totally\": B_INCR, \"tremendously\": B_INCR,\n",
    " \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utterly\": B_INCR,\n",
    " \"very\": B_INCR,\n",
    " \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
    " \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
    " \"less\": B_DECR, \"little\": B_DECR, \"marginally\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
    " \"scarcely\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
    " \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n",
    "\n",
    "# check for special case idioms using a sentiment-laden keyword known to VADER\n",
    "SPECIAL_CASE_IDIOMS = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"yeah right\": -2,\n",
    "                       \"cut the mustard\": 2, \"kiss of death\": -1.5, \"hand to mouth\": -2}\n",
    "\n",
    "class SentiText(object):\n",
    "    \"\"\"\n",
    "    Identify sentiment-relevant string-level properties of input text.\n",
    "    \"\"\"\n",
    "    def __init__(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text.encode('utf-8'))\n",
    "        self.text = text\n",
    "        self.words_and_emoticons = self._words_and_emoticons()\n",
    "        # doesn't separate words from\\\n",
    "        # adjacent punctuation (keeps emoticons & contractions)\n",
    "        self.is_cap_diff = allcap_differential(self.words_and_emoticons)\n",
    "\n",
    "    def _words_plus_punc(self):\n",
    "        \"\"\"\n",
    "        Returns mapping of form:\n",
    "        {\n",
    "            'cat,': 'cat',\n",
    "            ',cat': 'cat',\n",
    "        }\n",
    "        \"\"\"\n",
    "        no_punc_text = REGEX_REMOVE_PUNCTUATION.sub('', self.text)\n",
    "        # removes punctuation (but loses emoticons & contractions)\n",
    "        words_only = no_punc_text.split()\n",
    "        # remove singletons\n",
    "        words_only = set( w for w in words_only if len(w) > 1 )\n",
    "        # the product gives ('cat', ',') and (',', 'cat')\n",
    "        punc_before = {''.join(p): p[1] for p in product(PUNC_LIST, words_only)}\n",
    "        punc_after = {''.join(p): p[0] for p in product(words_only, PUNC_LIST)}\n",
    "        words_punc_dict = punc_before\n",
    "        words_punc_dict.update(punc_after)\n",
    "        return words_punc_dict\n",
    "\n",
    "    def _words_and_emoticons(self):\n",
    "        \"\"\"\n",
    "        Removes leading and trailing puncutation\n",
    "        Leaves contractions and most emoticons\n",
    "            Does not preserve punc-plus-letter emoticons (e.g. :D)\n",
    "        \"\"\"\n",
    "        wes = self.text.split()\n",
    "        words_punc_dict = self._words_plus_punc()\n",
    "        wes = [we for we in wes if len(we) > 1]\n",
    "        for i, we in enumerate(wes):\n",
    "            if we in words_punc_dict:\n",
    "                wes[i] = words_punc_dict[we]\n",
    "        return wes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = list(df['review_text'])\n",
    "\n",
    "compound = []\n",
    "positive =[]\n",
    "neutral=[]\n",
    "negative =[]\n",
    "\n",
    "for r in reviews:\n",
    "    vs = analyzer.polarity_scores(r)\n",
    "    compound.append(vs['compound'])\n",
    "    positive.append(vs['pos'])\n",
    "    neutral.append(vs['neu'])\n",
    "    negative.append(vs['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>story_id</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>compound</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3522972</td>\n",
       "      <td>11-04-14</td>\n",
       "      <td>Why is this so adorable? I blame the ending. S...</td>\n",
       "      <td>10000529</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.7868</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3700017</td>\n",
       "      <td>07-02-14</td>\n",
       "      <td>Oh my goodness. Eliot and Clint working togeth...</td>\n",
       "      <td>10000529</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>462777</td>\n",
       "      <td>1/30/2014</td>\n",
       "      <td>This was an awesome piece! I like how you bro...</td>\n",
       "      <td>10000529</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7455526</td>\n",
       "      <td>05-abr</td>\n",
       "      <td>wtf</td>\n",
       "      <td>10001611</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5859</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6451578</td>\n",
       "      <td>12/24/2016</td>\n",
       "      <td>I just read this a second time &amp; I still think...</td>\n",
       "      <td>10001611</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_name review_date  \\\n",
       "0        3522972    11-04-14   \n",
       "1        3700017    07-02-14   \n",
       "2         462777   1/30/2014   \n",
       "3        7455526      05-abr   \n",
       "4        6451578  12/24/2016   \n",
       "\n",
       "                                         review_text  story_id  negative  \\\n",
       "0  Why is this so adorable? I blame the ending. S...  10000529     0.158   \n",
       "1  Oh my goodness. Eliot and Clint working togeth...  10000529     0.000   \n",
       "2   This was an awesome piece! I like how you bro...  10000529     0.000   \n",
       "3                                                wtf  10001611     1.000   \n",
       "4  I just read this a second time & I still think...  10001611     0.000   \n",
       "\n",
       "   neutral  positive  compound   emotion  \n",
       "0    0.469     0.373    0.7868  positive  \n",
       "1    0.638     0.362    0.7783  positive  \n",
       "2    0.733     0.267    0.8805  positive  \n",
       "3    0.000     0.000   -0.5859  negative  \n",
       "4    1.000     0.000    0.0000  negative  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['negative'] = pd.Series(negative)\n",
    "df['neutral'] = pd.Series(neutral)\n",
    "df['positive'] = pd.Series(positive)\n",
    "df['compound'] = pd.Series(compound)\n",
    "df['emotion'] = np.where(df['compound']>0.65, 'positive', 'negative')\n",
    "\n",
    "# guardamos csv con reviews y sentiment analysis de c/u \n",
    "#df.to_csv('reviews_sentiment.csv', sep='|', index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sentiment analysis stories summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>story_id</th>\n",
       "      <th>plays</th>\n",
       "      <th>franchise_x</th>\n",
       "      <th>story_title</th>\n",
       "      <th>author</th>\n",
       "      <th>franchise_y</th>\n",
       "      <th>story_summary</th>\n",
       "      <th>q_words</th>\n",
       "      <th>date_submit</th>\n",
       "      <th>date_update</th>\n",
       "      <th>chapters</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>752508</td>\n",
       "      <td>1984041</td>\n",
       "      <td>1</td>\n",
       "      <td>Alias</td>\n",
       "      <td>Trying to Carry On</td>\n",
       "      <td>590553.0</td>\n",
       "      <td>Alias</td>\n",
       "      <td>He had left her, her and her daughter. She wan...</td>\n",
       "      <td>51285.0</td>\n",
       "      <td>7/27/2004,9/6/2004,3/19/2005,9/6/2004,7/21/200...</td>\n",
       "      <td>11/24/2005</td>\n",
       "      <td>17.0</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>752508</td>\n",
       "      <td>2046469</td>\n",
       "      <td>1</td>\n",
       "      <td>Alias</td>\n",
       "      <td>A Second Chance</td>\n",
       "      <td>590553.0</td>\n",
       "      <td>Alias</td>\n",
       "      <td>SV had an affair and as a result, Sydney becam...</td>\n",
       "      <td>34944.0</td>\n",
       "      <td>7/27/2004,9/6/2004,3/19/2005,9/6/2004,7/21/200...</td>\n",
       "      <td>7/22/2005</td>\n",
       "      <td>11.0</td>\n",
       "      <td>238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>752508</td>\n",
       "      <td>1991603</td>\n",
       "      <td>1</td>\n",
       "      <td>Alias</td>\n",
       "      <td>A Question of Fate</td>\n",
       "      <td>586795.0</td>\n",
       "      <td>Alias</td>\n",
       "      <td>SV S3.  Angst, Romance, Action, Humor aw, come...</td>\n",
       "      <td>52468.0</td>\n",
       "      <td>2/3/2005,8/1/2004,10/28/2004</td>\n",
       "      <td>6/16/2005</td>\n",
       "      <td>21.0</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>752508</td>\n",
       "      <td>2179554</td>\n",
       "      <td>1</td>\n",
       "      <td>Alias</td>\n",
       "      <td>The Ace of Hearts</td>\n",
       "      <td>590553.0</td>\n",
       "      <td>Alias</td>\n",
       "      <td>Sydney and Vaughn play a card game to pass the...</td>\n",
       "      <td>17342.0</td>\n",
       "      <td>7/27/2004,9/6/2004,3/19/2005,9/6/2004,7/21/200...</td>\n",
       "      <td>5/6/2005</td>\n",
       "      <td>5.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>752508</td>\n",
       "      <td>1879281</td>\n",
       "      <td>1</td>\n",
       "      <td>Alias</td>\n",
       "      <td>The Vaughn\\'s</td>\n",
       "      <td>251435.0</td>\n",
       "      <td>Alias</td>\n",
       "      <td>Sequel to 'One wedding and a near funeral.'  S...</td>\n",
       "      <td>26402.0</td>\n",
       "      <td>5/26/2004,11/15/2004,4/21/2004,3/17/2004,12/12...</td>\n",
       "      <td>2/21/2005</td>\n",
       "      <td>20.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user  story_id  plays franchise_x         story_title    author  \\\n",
       "0  752508   1984041      1       Alias  Trying to Carry On  590553.0   \n",
       "1  752508   2046469      1       Alias     A Second Chance  590553.0   \n",
       "2  752508   1991603      1       Alias  A Question of Fate  586795.0   \n",
       "3  752508   2179554      1       Alias   The Ace of Hearts  590553.0   \n",
       "4  752508   1879281      1       Alias       The Vaughn\\'s  251435.0   \n",
       "\n",
       "  franchise_y                                      story_summary  q_words  \\\n",
       "0       Alias  He had left her, her and her daughter. She wan...  51285.0   \n",
       "1       Alias  SV had an affair and as a result, Sydney becam...  34944.0   \n",
       "2       Alias  SV S3.  Angst, Romance, Action, Humor aw, come...  52468.0   \n",
       "3       Alias  Sydney and Vaughn play a card game to pass the...  17342.0   \n",
       "4       Alias  Sequel to 'One wedding and a near funeral.'  S...  26402.0   \n",
       "\n",
       "                                         date_submit date_update  chapters  \\\n",
       "0  7/27/2004,9/6/2004,3/19/2005,9/6/2004,7/21/200...  11/24/2005      17.0   \n",
       "1  7/27/2004,9/6/2004,3/19/2005,9/6/2004,7/21/200...   7/22/2005      11.0   \n",
       "2                       2/3/2005,8/1/2004,10/28/2004   6/16/2005      21.0   \n",
       "3  7/27/2004,9/6/2004,3/19/2005,9/6/2004,7/21/200...    5/6/2005       5.0   \n",
       "4  5/26/2004,11/15/2004,4/21/2004,3/17/2004,12/12...   2/21/2005      20.0   \n",
       "\n",
       "   reviews  \n",
       "0    267.0  \n",
       "1    238.0  \n",
       "2    122.0  \n",
       "3    138.0  \n",
       "4     55.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = pd.read_csv('datasets_recsys/ff_users_fav_stories_data.csv', sep='|', encoding='latin')\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = list(df_summary['story_summary'])\n",
    "\n",
    "compound = []\n",
    "positive =[]\n",
    "neutral=[]\n",
    "negative =[]\n",
    "\n",
    "for r in summary:\n",
    "    try:\n",
    "        vs = analyzer.polarity_scores(r)\n",
    "        compound.append(vs['compound'])\n",
    "        positive.append(vs['pos'])\n",
    "        neutral.append(vs['neu'])\n",
    "        negative.append(vs['neg'])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary['negative_sum'] = pd.Series(negative)\n",
    "df_summary['neutral_sum'] = pd.Series(neutral)\n",
    "df_summary['positive_sum'] = pd.Series(positive)\n",
    "df_summary['compound_sum'] = pd.Series(compound)\n",
    "df_summary['emotion_sum'] = np.where(df_summary['compound_sum']>0.65, 'positive', 'negative')\n",
    "\n",
    "df_summary.head(10)\n",
    "\n",
    "# reordenamos \n",
    "df_summary = df_summary[['story_id', 'author','franchise', 'story_title','story_summary', 'q_words', 'reviews', 'compound_sum', 'emotion_sum']]\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ahora hacemos una comparacion de sentiment analysis entre reviews y resumen de historias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total stories: 139754\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>story_id</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>compound</th>\n",
       "      <th>emotion</th>\n",
       "      <th>author</th>\n",
       "      <th>franchise</th>\n",
       "      <th>story_title</th>\n",
       "      <th>story_summary</th>\n",
       "      <th>q_words</th>\n",
       "      <th>reviews</th>\n",
       "      <th>compound_sum</th>\n",
       "      <th>emotion_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3522972</td>\n",
       "      <td>11-04-14</td>\n",
       "      <td>Why is this so adorable? I blame the ending. S...</td>\n",
       "      <td>10000529</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.7868</td>\n",
       "      <td>positive</td>\n",
       "      <td>4685422.0</td>\n",
       "      <td>Leverage &amp; Avengers</td>\n",
       "      <td>A Little Way Up The Road</td>\n",
       "      <td>Clint asks an old friend for some assistance. ...</td>\n",
       "      <td>1793.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3700017</td>\n",
       "      <td>07-02-14</td>\n",
       "      <td>Oh my goodness. Eliot and Clint working togeth...</td>\n",
       "      <td>10000529</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>positive</td>\n",
       "      <td>4685422.0</td>\n",
       "      <td>Leverage &amp; Avengers</td>\n",
       "      <td>A Little Way Up The Road</td>\n",
       "      <td>Clint asks an old friend for some assistance. ...</td>\n",
       "      <td>1793.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>462777</td>\n",
       "      <td>1/30/2014</td>\n",
       "      <td>This was an awesome piece! I like how you bro...</td>\n",
       "      <td>10000529</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>positive</td>\n",
       "      <td>4685422.0</td>\n",
       "      <td>Leverage &amp; Avengers</td>\n",
       "      <td>A Little Way Up The Road</td>\n",
       "      <td>Clint asks an old friend for some assistance. ...</td>\n",
       "      <td>1793.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7455526</td>\n",
       "      <td>05-abr</td>\n",
       "      <td>wtf</td>\n",
       "      <td>10001611</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5859</td>\n",
       "      <td>negative</td>\n",
       "      <td>5439324.0</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Life\\'s Too Short</td>\n",
       "      <td>Anna wants to know why Elsa keeps turning down...</td>\n",
       "      <td>8806.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6451578</td>\n",
       "      <td>12/24/2016</td>\n",
       "      <td>I just read this a second time &amp; I still think...</td>\n",
       "      <td>10001611</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negative</td>\n",
       "      <td>5439324.0</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Life\\'s Too Short</td>\n",
       "      <td>Anna wants to know why Elsa keeps turning down...</td>\n",
       "      <td>8806.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_name review_date  \\\n",
       "0        3522972    11-04-14   \n",
       "1        3700017    07-02-14   \n",
       "2         462777   1/30/2014   \n",
       "3        7455526      05-abr   \n",
       "4        6451578  12/24/2016   \n",
       "\n",
       "                                         review_text  story_id  negative  \\\n",
       "0  Why is this so adorable? I blame the ending. S...  10000529     0.158   \n",
       "1  Oh my goodness. Eliot and Clint working togeth...  10000529     0.000   \n",
       "2   This was an awesome piece! I like how you bro...  10000529     0.000   \n",
       "3                                                wtf  10001611     1.000   \n",
       "4  I just read this a second time & I still think...  10001611     0.000   \n",
       "\n",
       "   neutral  positive  compound   emotion     author            franchise  \\\n",
       "0    0.469     0.373    0.7868  positive  4685422.0  Leverage & Avengers   \n",
       "1    0.638     0.362    0.7783  positive  4685422.0  Leverage & Avengers   \n",
       "2    0.733     0.267    0.8805  positive  4685422.0  Leverage & Avengers   \n",
       "3    0.000     0.000   -0.5859  negative  5439324.0               Frozen   \n",
       "4    1.000     0.000    0.0000  negative  5439324.0               Frozen   \n",
       "\n",
       "                story_title  \\\n",
       "0  A Little Way Up The Road   \n",
       "1  A Little Way Up The Road   \n",
       "2  A Little Way Up The Road   \n",
       "3         Life\\'s Too Short   \n",
       "4         Life\\'s Too Short   \n",
       "\n",
       "                                       story_summary  q_words  reviews  \\\n",
       "0  Clint asks an old friend for some assistance. ...   1793.0      5.0   \n",
       "1  Clint asks an old friend for some assistance. ...   1793.0      5.0   \n",
       "2  Clint asks an old friend for some assistance. ...   1793.0      5.0   \n",
       "3  Anna wants to know why Elsa keeps turning down...   8806.0     57.0   \n",
       "4  Anna wants to know why Elsa keeps turning down...   8806.0     57.0   \n",
       "\n",
       "   compound_sum emotion_sum  \n",
       "0        0.8689    positive  \n",
       "1        0.8689    positive  \n",
       "2        0.8689    positive  \n",
       "3        0.1260    negative  \n",
       "4        0.1260    negative  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = pd.merge(df, df_summary, on=['story_id'])\n",
    "\n",
    "print('total stories: {}'.format(len(df_total)))\n",
    "\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story_id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>compound</th>\n",
       "      <th>emotion</th>\n",
       "      <th>franchise</th>\n",
       "      <th>story_summary</th>\n",
       "      <th>compound_sum</th>\n",
       "      <th>emotion_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000529</td>\n",
       "      <td>Why is this so adorable? I blame the ending. S...</td>\n",
       "      <td>0.7868</td>\n",
       "      <td>positive</td>\n",
       "      <td>Leverage &amp; Avengers</td>\n",
       "      <td>Clint asks an old friend for some assistance. ...</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000529</td>\n",
       "      <td>Oh my goodness. Eliot and Clint working togeth...</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>positive</td>\n",
       "      <td>Leverage &amp; Avengers</td>\n",
       "      <td>Clint asks an old friend for some assistance. ...</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000529</td>\n",
       "      <td>This was an awesome piece! I like how you bro...</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>positive</td>\n",
       "      <td>Leverage &amp; Avengers</td>\n",
       "      <td>Clint asks an old friend for some assistance. ...</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001611</td>\n",
       "      <td>wtf</td>\n",
       "      <td>-0.5859</td>\n",
       "      <td>negative</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Anna wants to know why Elsa keeps turning down...</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001611</td>\n",
       "      <td>I just read this a second time &amp; I still think...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negative</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Anna wants to know why Elsa keeps turning down...</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   story_id                                        review_text  compound  \\\n",
       "0  10000529  Why is this so adorable? I blame the ending. S...    0.7868   \n",
       "1  10000529  Oh my goodness. Eliot and Clint working togeth...    0.7783   \n",
       "2  10000529   This was an awesome piece! I like how you bro...    0.8805   \n",
       "3  10001611                                                wtf   -0.5859   \n",
       "4  10001611  I just read this a second time & I still think...    0.0000   \n",
       "\n",
       "    emotion            franchise  \\\n",
       "0  positive  Leverage & Avengers   \n",
       "1  positive  Leverage & Avengers   \n",
       "2  positive  Leverage & Avengers   \n",
       "3  negative               Frozen   \n",
       "4  negative               Frozen   \n",
       "\n",
       "                                       story_summary  compound_sum emotion_sum  \n",
       "0  Clint asks an old friend for some assistance. ...        0.8689    positive  \n",
       "1  Clint asks an old friend for some assistance. ...        0.8689    positive  \n",
       "2  Clint asks an old friend for some assistance. ...        0.8689    positive  \n",
       "3  Anna wants to know why Elsa keeps turning down...        0.1260    negative  \n",
       "4  Anna wants to know why Elsa keeps turning down...        0.1260    negative  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = df_total[['story_id','review_text','compound','emotion','franchise','story_summary','compound_sum','emotion_sum']]\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coincidencias sentiment reviews/summary: 79590\n",
      "coincidencia porcentual sentiment summary/reviews: 56.95006940767349 %\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for x in range(0,len(list(df_total['emotion']))):\n",
    "    if list(df_total['emotion'])[x] == list(df_total['emotion_sum'])[x] :\n",
    "        count+=1\n",
    "\n",
    "print('coincidencias sentiment reviews/summary: {}'.format(count))\n",
    "print('coincidencia porcentual sentiment summary/reviews: {} %'.format((count/len(list(df_total['emotion'])))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentimiento del resumen de las franquicias: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentimiento promedio de franchises summary (sample 50 franquicias aleatorias)\n",
    "franchise_sentiment = df_summary[['story_id','franchise_x','compound_sum']].sample(30)\n",
    "franchise_sentiment = franchise_sentiment.groupby(['franchise_x'])['compound_sum'].mean()\n",
    "\n",
    "franchise_sentiment.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentimiento de las reviews en las franquicias: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2acdf361eac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sentimiento promedio de las reviews franchises (random 50 franquicias)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfranchise_sentiment_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'story_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'franchise'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfranchise_sentiment_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfranchise_sentiment_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'franchise'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfranchise_sentiment_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_total' is not defined"
     ]
    }
   ],
   "source": [
    "# sentimiento promedio de las reviews franchises (random 50 franquicias)\n",
    "franchise_sentiment_reviews = df_total[['story_id','franchise','compound']].sample(30)\n",
    "franchise_sentiment_reviews = franchise_sentiment_reviews.groupby(['franchise'])['compound'].mean()\n",
    "franchise_sentiment_reviews.plot.bar(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    139754.000000\n",
       "mean          0.398409\n",
       "std           0.473383\n",
       "min          -1.000000\n",
       "25%           0.000000\n",
       "50%           0.540000\n",
       "75%           0.796400\n",
       "max           1.000000\n",
       "Name: compound, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentimientos de los reviews \n",
    "df_total['compound'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    139754.000000\n",
       "mean          0.060523\n",
       "std           0.566547\n",
       "min          -0.986400\n",
       "25%          -0.421500\n",
       "50%           0.000000\n",
       "75%           0.571900\n",
       "max           0.989600\n",
       "Name: compound_sum, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentimientos de los resumenes de las historias \n",
    "df_total['compound_sum'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Los reviewers tienen pensamientos mas positivos que los mismos escritores de las historias en fanfiction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
